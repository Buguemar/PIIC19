{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "        \n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    ####### PARA MEDIR F1 AL FINA LDE EPOCH ###########\n",
    "    class Metrics(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.val_f1s = []\n",
    "\n",
    "        def on_epoch_end(self, batch, logs={}):\n",
    "            predict = np.squeeze(self.model.predict_classes(self.validation_data[0]))\n",
    "            targ = np.squeeze(self.validation_data[1])\n",
    "            f1s = f1_score(targ, predict, average='macro')\n",
    "            self.val_f1s.append(f1s)\n",
    "            #print(\" - val_f1: %f \" %(f1s))\n",
    "            return\n",
    "\n",
    "    from keras import backend as K\n",
    "    def f1(y_true, y_pred):\n",
    "        def recall(y_true, y_pred):\n",
    "            \"\"\"Recall metric.\n",
    "\n",
    "            Only computes a batch-wise average of recall.\n",
    "\n",
    "            Computes the recall, a metric for multi-label classification of\n",
    "            how many relevant items are selected.\n",
    "            \"\"\"\n",
    "            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "        def precision(y_true, y_pred):\n",
    "            \"\"\"Precision metric.\n",
    "\n",
    "            Only computes a batch-wise average of precision.\n",
    "\n",
    "            Computes the precision, a metric for multi-label classification of\n",
    "            how many selected items are relevant.\n",
    "            \"\"\"\n",
    "            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "        precision = precision(y_true, y_pred)\n",
    "        recall = recall(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D({{choice([32,64,128,256, 512])}}, (3, 3), input_shape=x_train.shape[1:], strides=(1, 1), padding=\"valid\"))\n",
    "    #model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    model.add(Conv2D({{choice([32,64,128,256, 512])}}, (3, 3), strides=(1, 1), padding=\"valid\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    if {{choice(['two', 'three'])}} == 'three': #if three add a third conv layer\n",
    "        model.add(Conv2D({{choice([32,64,128,256, 512])}}, (3, 3), strides=(1, 1), padding=\"valid\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #model.add(GlobalAveragePooling2D()) ## en otro kernel..\n",
    "\n",
    "    model.add(Dense({{choice([64,128,256, 512,1024])}}))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    if {{choice(['one', 'two'])}} == 'two': #if two add a second dense layer on flatten\n",
    "        model.add(Dense({{choice([64,128,256, 512,1024])}}))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='Adam')#, metrics=['acc'])\n",
    "    metrics = Metrics()\n",
    "    result = model.fit(x_train, y_train,\n",
    "                  batch_size={{choice([64, 128,256])}},\n",
    "                  epochs={{choice([60,200])}},\n",
    "                  verbose=0,\n",
    "                  validation_split=0.1, callbacks=[metrics])\n",
    "\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(metrics.val_f1s) #np.amax(result.history['val_f1']) \n",
    "    print('Best validation metric of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score,f1_score, confusion_matrix, mean_absolute_error,mean_squared_error, median_absolute_error\n",
    "from sklearn.preprocessing import normalize\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_df(df):\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df)\n",
    "    except:\n",
    "        print(df)\n",
    "    \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix (f1-score)',cmap=None, normalize=True):\n",
    "    import itertools\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0.0, vmax=1.0)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_metrics(y_true,y_pred,plot=True, title=\"\"):\n",
    "    dic_return = {}\n",
    "    dic_return[\"Precision raw\"] = precision_score(y_true,y_pred,average=None,labels=[0,1])\n",
    "    dic_return[\"Recall raw\"] = recall_score(y_true,y_pred,average=None,labels=[0,1])\n",
    "    #dic_return[\"Precision\"] = precision_score(y_true,y_pred,average=None,labels=[0,1])\n",
    "    #dic_return[\"Recall\"] = recall_score(y_true,y_pred,average=None,labels=[0,1])\n",
    "    dic_return[\"F1 raw\"] = f1_score(y_true,y_pred,average=None,labels=[0,1])\n",
    "    dic_return[\"F1 weighted\"] = f1_score(y_true,y_pred,average=\"weighted\",labels=[0,1])\n",
    "    dic_return[\"F1 macro\"] = f1_score(y_true,y_pred,average=\"macro\",labels=[0,1])\n",
    "    dic_return[\"F1 micro\"] = f1_score(y_true,y_pred,average=\"micro\",labels=[0,1])\n",
    "    matriz=confusion_matrix(y_true,y_pred, labels=[0,1])\n",
    "    if plot:\n",
    "        df = pd.DataFrame(dic_return)\n",
    "        df.index = [\"False Positive\",\"Confirmed\"]\n",
    "        plot_df(df)\n",
    "        plot_confusion_matrix(normalize(matriz,axis=1,norm='l1'),[\"False Positive\",\"Confirmed\"],title)\n",
    "    dic_return[\"Confusion Matrix\"] = matriz\n",
    "    return dic_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    channel_1 = np.load('/work/work_teamEXOPLANET/MTF_margarita/no_invertidos/MTF_u15_d15.npy')  \n",
    "    channel_2 = np.load('/work/work_teamEXOPLANET/MTF_margarita/invertidos/MTF_u15_d15.npy')  \n",
    "    channel_3 = np.load('/work/work_teamEXOPLANET/MTF_gabo/npys/time_channel_30.npy')  \n",
    "\n",
    "    x_all = []\n",
    "    for i in range(channel_1.shape[0]):\n",
    "        combined_image = np.dstack((channel_1[i],channel_2[i],channel_3[i]))\n",
    "        x_all.append(combined_image)\n",
    "    x_all = np.asarray(x_all)\n",
    "\n",
    "    fold =\"../../KOI_Data/\"\n",
    "    df_sets = pd.read_csv(fold+\"/koi_sets.csv\") \n",
    "    mask_train = (df_sets[\"Set\"] == \"Train\").values\n",
    "    mask_test = (df_sets[\"Set\"] == \"Test\").values\n",
    "\n",
    "    df_meta = pd.read_csv(fold+\"/koi_metadata.csv\")\n",
    "    df_meta_train = df_meta[mask_train]\n",
    "    df_meta_test = df_meta[mask_test]\n",
    "    y_train= (df_meta_train['NExScI Disposition'].values==\"CONFIRMED\")*1\n",
    "    y_val = (df_meta_test['NExScI Disposition'].values==\"CONFIRMED\")*1\n",
    "\n",
    "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "    unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
    "\n",
    "    x_train = x_all[mask_train]\n",
    "    x_val = x_all[mask_test]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import KFold\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, mean_absolute_error, mean_squared_error, median_absolute_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import normalize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import display\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import itertools\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Conv2D': hp.choice('Conv2D', [32,64,128,256, 512]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Conv2D_1': hp.choice('Conv2D_1', [32,64,128,256, 512]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dropout_2': hp.choice('Dropout_2', ['two', 'three']),\n",
      "        'Conv2D_2': hp.choice('Conv2D_2', [32,64,128,256, 512]),\n",
      "        'Dropout_3': hp.uniform('Dropout_3', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [64,128,256, 512,1024]),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'Dropout_5': hp.choice('Dropout_5', ['one', 'two']),\n",
      "        'Dense_1': hp.choice('Dense_1', [64,128,256, 512,1024]),\n",
      "        'Dropout_6': hp.uniform('Dropout_6', 0, 1),\n",
      "        'batch_size': hp.choice('batch_size', [64, 128,256]),\n",
      "        'epochs': hp.choice('epochs', [60,200]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\"\n",
      "   3: Data providing function:\n",
      "   4: \n",
      "   5: This function is separated from create_model() so that hyperopt\n",
      "   6: won't reload data for each evaluation run.\n",
      "   7: \"\"\"\n",
      "   8: channel_1 = np.load('/work/work_teamEXOPLANET/MTF_margarita/no_invertidos/MTF_u15_d15.npy')  \n",
      "   9: channel_2 = np.load('/work/work_teamEXOPLANET/MTF_margarita/invertidos/MTF_u15_d15.npy')  \n",
      "  10: channel_3 = np.load('/work/work_teamEXOPLANET/MTF_gabo/npys/time_channel_30.npy')  \n",
      "  11: \n",
      "  12: x_all = []\n",
      "  13: for i in range(channel_1.shape[0]):\n",
      "  14:     combined_image = np.dstack((channel_1[i],channel_2[i],channel_3[i]))\n",
      "  15:     x_all.append(combined_image)\n",
      "  16: x_all = np.asarray(x_all)\n",
      "  17: \n",
      "  18: fold =\"../../KOI_Data/\"\n",
      "  19: df_sets = pd.read_csv(fold+\"/koi_sets.csv\") \n",
      "  20: mask_train = (df_sets[\"Set\"] == \"Train\").values\n",
      "  21: mask_test = (df_sets[\"Set\"] == \"Test\").values\n",
      "  22: \n",
      "  23: df_meta = pd.read_csv(fold+\"/koi_metadata.csv\")\n",
      "  24: df_meta_train = df_meta[mask_train]\n",
      "  25: df_meta_test = df_meta[mask_test]\n",
      "  26: y_train= (df_meta_train['NExScI Disposition'].values==\"CONFIRMED\")*1\n",
      "  27: y_val = (df_meta_test['NExScI Disposition'].values==\"CONFIRMED\")*1\n",
      "  28: \n",
      "  29: unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
      "  30: unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
      "  31: \n",
      "  32: x_train = x_all[mask_train]\n",
      "  33: x_val = x_all[mask_test]\n",
      "  34: \n",
      "  35: \n",
      "  36: \n",
      "  37: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     ####### PARA MEDIR F1 AL FINA LDE EPOCH ###########\n",
      "   4:     class Metrics(keras.callbacks.Callback):\n",
      "   5:         def on_train_begin(self, logs={}):\n",
      "   6:             self.val_f1s = []\n",
      "   7: \n",
      "   8:         def on_epoch_end(self, batch, logs={}):\n",
      "   9:             predict = np.squeeze(self.model.predict_classes(self.validation_data[0]))\n",
      "  10:             targ = np.squeeze(self.validation_data[1])\n",
      "  11:             f1s = f1_score(targ, predict, average='macro')\n",
      "  12:             self.val_f1s.append(f1s)\n",
      "  13:             #print(\" - val_f1: %f \" %(f1s))\n",
      "  14:             return\n",
      "  15: \n",
      "  16:     def f1(y_true, y_pred):\n",
      "  17:         def recall(y_true, y_pred):\n",
      "  18:             \"\"\"Recall metric.\n",
      "  19: \n",
      "  20:             Only computes a batch-wise average of recall.\n",
      "  21: \n",
      "  22:             Computes the recall, a metric for multi-label classification of\n",
      "  23:             how many relevant items are selected.\n",
      "  24:             \"\"\"\n",
      "  25:             true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
      "  26:             possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
      "  27:             recall = true_positives / (possible_positives + K.epsilon())\n",
      "  28:             return recall\n",
      "  29: \n",
      "  30:         def precision(y_true, y_pred):\n",
      "  31:             \"\"\"Precision metric.\n",
      "  32: \n",
      "  33:             Only computes a batch-wise average of precision.\n",
      "  34: \n",
      "  35:             Computes the precision, a metric for multi-label classification of\n",
      "  36:             how many selected items are relevant.\n",
      "  37:             \"\"\"\n",
      "  38:             true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
      "  39:             predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
      "  40:             precision = true_positives / (predicted_positives + K.epsilon())\n",
      "  41:             return precision\n",
      "  42:         precision = precision(y_true, y_pred)\n",
      "  43:         recall = recall(y_true, y_pred)\n",
      "  44:         return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
      "  45: \n",
      "  46:     model = Sequential()\n",
      "  47:     model.add(Conv2D(space['Conv2D'], (3, 3), input_shape=x_train.shape[1:], strides=(1, 1), padding=\"valid\"))\n",
      "  48:     #model.add(Activation(space['Activation']))\n",
      "  49:     model.add(Activation('relu'))\n",
      "  50:     model.add(MaxPooling2D((2,2)))\n",
      "  51:     model.add(Dropout(space['Dropout']))\n",
      "  52: \n",
      "  53:     model.add(Conv2D(space['Conv2D_1'], (3, 3), strides=(1, 1), padding=\"valid\"))\n",
      "  54:     model.add(Activation('relu'))\n",
      "  55:     model.add(MaxPooling2D((2,2)))\n",
      "  56:     model.add(Dropout(space['Dropout_1']))\n",
      "  57: \n",
      "  58:     if space['Dropout_2'] == 'three': #if three add a third conv layer\n",
      "  59:         model.add(Conv2D(space['Conv2D_2'], (3, 3), strides=(1, 1), padding=\"valid\"))\n",
      "  60:         model.add(Activation('relu'))\n",
      "  61:         model.add(MaxPooling2D((2,2)))\n",
      "  62:         model.add(Dropout(space['Dropout_3']))\n",
      "  63: \n",
      "  64:     model.add(Flatten())\n",
      "  65:     #model.add(GlobalAveragePooling2D()) ## en otro kernel..\n",
      "  66: \n",
      "  67:     model.add(Dense(space['Dense']))\n",
      "  68:     model.add(Activation('relu'))\n",
      "  69:     model.add(Dropout(space['Dropout_4']))\n",
      "  70:     \n",
      "  71:     if space['Dropout_5'] == 'two': #if two add a second dense layer on flatten\n",
      "  72:         model.add(Dense(space['Dense_1']))\n",
      "  73:         model.add(Activation('relu'))\n",
      "  74:         model.add(Dropout(space['Dropout_6']))\n",
      "  75:         \n",
      "  76:     model.add(Dense(1, activation='sigmoid'))\n",
      "  77:     model.summary()\n",
      "  78: \n",
      "  79:     model.compile(loss='binary_crossentropy',optimizer='Adam')#, metrics=['acc'])\n",
      "  80:     metrics = Metrics()\n",
      "  81:     result = model.fit(x_train, y_train,\n",
      "  82:                   batch_size=space['batch_size'],\n",
      "  83:                   epochs=space['epochs'],\n",
      "  84:                   verbose=0,\n",
      "  85:                   validation_split=0.1, callbacks=[metrics])\n",
      "  86: \n",
      "  87:     #get the highest validation accuracy of the training epochs\n",
      "  88:     validation_acc = np.amax(metrics.val_f1s) #np.amax(result.history['val_f1']) \n",
      "  89:     print('Best validation metric of epoch:', validation_acc)\n",
      "  90:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  91: \n",
      "  0%|          | 0/60 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.734215 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.651797 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_1\"                               \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 256)       7168      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                589888    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,191,361                             \n",
      "Trainable params: 1,191,361                         \n",
      "Non-trainable params: 0                             \n",
      "_________________________________________________________________\n",
      "  0%|          | 0/60 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ohpc/pub/moduledeps/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=60,\n",
    "                                          trials=Trials(),\n",
    "                                     notebook_name='Hyperkeras_f1fxed')\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_val, y_val))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val = data()\n",
    "\n",
    "predichos=best_model.predict_classes(x_val)\n",
    "calculate_metrics(y_val, predichos,'[VAL] Confusion Matrix 50 epochs CNN + reverse LC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
